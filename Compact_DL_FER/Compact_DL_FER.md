### A Compact Deep Learning Model for Robust Facial Expression Recognition
2018 CVPR  
### 紧凑的深度学习模型，用于鲁棒的面部表情识别   
#### 摘要：
我们提出了一种基于框架的紧凑型面部表情识别框架，用于面部表情识别，相比于最新方法具有非常有竞争力的性能，同时使用的参数更少。提出的框架扩展到frame-to-sequence方法,通过利用有gated recurrent units（门控循环单元）的时间信息。此外，我们开发了一种光照增强方案，以缓解使用混合数据源训练深度网络时的过拟合问题。最后，我们使用提出的技术在一些公共数据集上证明性能的改进。  
#### 1.Introduction  
随着深度学习和人机交互的进步，从图像中了解人的情感变得越来越重要。人的情感表达方式有多种。研究表明，非姿势表达的分析必须依靠其他生理信号，例如温度变化和心率。遗憾的是，这些生理方法在实际中通常不可用或不可行，这使得研究结果仅限于实验室环境中。  
由于易于获取数据，基于视频的方法最常用于表情识别。具有严格限制的数据库通常用于面部表情识别的性能基准测试。传统的基于图像的面部表情识别方法采用了手工特征，比如LBP，BoW,HoG,SIFT,并且它们在几个数据库上都显示了相当不错的结果。此外，基于序列的方法还对时态情绪变化建模，从视频中提取出时间时间手工特征。   
最近，自然环境下的表情识别引起了广泛的关注。这种问题很具有挑战性，因为收集的面部图像通常是从互联网获取的在不同的照明条件和头部姿势下。诸如EmotioNet之类的研究还表明，将下载的图像用于训练集对于提高模型训练的通用性非常有用。这激励我们进一步研究表情识别任务如何从不受约束的环境中获取的面部图像数据集的模型训练中受益。  
在本文中，我们介绍了一种新的卷积神经网络（CNN）架构，以通过适当的深层设计来提高性能和泛化能力网络。标准数据库上的实验还显示所提出的CNN模型适用于面部具有紧凑网络参数的表情识别与相关的基于深度学习的模型相比。 此外，我们将几个不同类型的数据集包含到训练数据集以提高学习到的CNN模型的泛化能力。除此之外，我们研究了一种光照增强方案，以提高训练的CNN模型的鲁棒性。本文的主要贡献可以总结如下：  
•提出了一种用于面部表情识别的紧凑型CNN模型，以在识别精度和模型大小之间折衷。  
•在两个标准数据库上评估了我们的网络模型，并证明了所提出的方法优于最新方法。  
•收集了三个不同场景的数据集可用于评估跨域性能。  
•我们提出的“一站式（leave-one-set-out）”实验表明提议的光照增强策略减轻了模型训练中不同来源的图像的过度拟合问题。  
#### 2. Related Work  
BDBN表明，特征提取和选择与统一的增强型深度置信网络相结合可获得更好的性能。 STM-ExpLet 使用expressionlet-based 时空流形为表情视频剪辑建模。Exemplar-HMMs结合HMMs和SVMs在基于模型的相似性框架中。LOMo结合不同类型的complimentary特征，比如面部标志，LBP，SIFT，和几何特征，FER。  
在最近几年，自从CNN在许多计算机视觉任务中展现出其空前的能力以来，深度学习就变得非常流行。已经针对不同的图像分类任务提出了各种CNN模型。 但是，这些深度网络不适用于小型表情识别数据库。  
联合微调方法采用具有7个不同旋转角度的数据扩充策略，以获得14倍的数据。根据外观和几何特征训练两个不同的网络，并通过联合微调来组合预训练的网络。研究人员还表明，将CNN与递归神经网络（RNN）结合使用对于从视频中进行表情识别非常有效。  
最近，通过将从大规模人脸识别数据库中学习到的知识进行转移，峰值引导方法成功地将GoogLeNet应用于表情识别。 他们的结果还表明，基于图像的方法的准确性与基于序列的方法的准确性相当。  
#### 3. Proposed Framework   
所提出的深度学习方法的总体流程如图1所示。我们的框架由两个模块组成：人脸预处理和CNN分类。为了确保我们的框架可以扩展到不同的场景，我们不采用任何时间标准化
方法。  ![Fig1](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Fig1.png)  
##### 3.1. Preprocessing   
我们首先根据IntraFace检测到的landmarks裁剪面部区域。这些landmarks可用于提取眉毛，眼睛，鼻子和嘴巴的轮廓。大crop可以保留更多信息，而小crop可以减少背景噪声或头部轮廓。裁剪后的图像尺寸$L=\alpha \times \max \left(d_{v}, d_{h}\right)$其中$d_{v}$是the uppermost landmark point和the lowermost landmark point之间的距离, $d_{h}$和the the leftmost landmark point和the rightmost landmark point之间的水平距离, α是用于控制面部区域大小的常量。对于所有实验，我们将α设置为1.05。  
一旦确定了裁剪的大小L，我们在鼻子的landmark上裁剪脸部区域中心，并获得适度的脸部图像以进行模型训练。将裁剪后的图像调整为固定尺寸120×120，然后将其发送到CNN分类器进行表情识别。  
##### 3.2. The CNN Model   
我们的CNN模型的架构如图2所示。 ![图二](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Fig2.png) 模型由两个卷积和池化块，然后是两个全连接层，使用ReLU作为每个卷积层的激活函数。在全连接层后应用下采样以防止过拟合。注意，该模型仅将调整后的面部图像的中心96×96部分用作输入。有关模型训练的详细信息将在下一部分中描述。  
提议的CNN结构可以视为DTAN的改进版本。他们的实验已经表明，该简单模型可以在表情识别任务中取得良好的效果。为了进一步提高模型的识别能力，我们在最大池化之前堆叠了两个连续的卷积层。我们还使用了较大的卷积滤波器，从而使模型中的神经元具有更大的感受野。经过这种修改后，第一个完全连接层中每个神经元的感受野将变为36×36，约为输入96×96图像的14％，而原来的DTGAN为16×16，仅占输入大小为64×64的6%。  
另一个重要的修改是减少完全连接的神经元的数量。我们相信，只要我们对感受野进行适当的设计，就可以通过适度的模型大小来学习人脸的表情。本文后面的实验表明，合适的轻量级全连接网络不仅模型参数紧凑，而且对于面部表情识别也很准确。  
#### 3.3. The Frame-to-Sequence Model  
标准面部表情数据库中的图像序列通常以中性表情开始，然后逐渐发展为峰值表情。我们可以用一个模型$S(x)$来近似这个转换过程，使用一系列图像$x_{i}^{t}, t=1, \ldots, T$作为输入，并且将每个图像序列映射到其ground truth$y_{i}$尽可能近：  
$
y_{i} \cong \widetilde{Y}_ {i}=S\left(x_{i}^{1}, \ldots, x_{i}^{T} ; \theta\right), --(1)
$   
T是图像序列的长度，$\theta$是一组模型参数。$p$表示由序列模型产生的每个表情的概率，序列建模问题可以表述为在给定训练序列的情况下最大化模型的对数似然性，即，  
$$
\hat{\theta}=\underset{\theta}{\arg \max } \frac{1}{N} \sum_{i=1}^{N} \log p\left(\widetilde{Y}_ {i} | x_{i}^{1}, \ldots, x_ {i}^{T} ; \theta\right), --(2)
$$  
这个问题很难直接解决，因此我们使用预先训练的CNN作为特征提取器。先前的基于帧的方法可以视为映射函数$F(x)$，映射每个$x_{i}^{t}$到概率分布 
$\{p_{1}^{t}(j), j=1, \dots, m\} $所以最大概率索引$p_{i}^{t}(j), \widetilde{y}_ {i}$像正确类别$y_{i}$一样,如  
$$y_{i} \cong \widetilde{y}_ {i}=\underset{j}{\arg \max } p_{i}^{t}(j)=F\left(x_{i}\right), --(3)$$  
$$p_{i}^{t}=F\left(x_{i}\right)=\left[p_{i}^{t}(1), p_{i}^{t}(2), \ldots, p_{i}^{t}(m)\right]$$  
这里，我们使用从CNNs计算出的概率分布序列进行基于帧的表情识别，而不是使用图像作为表情识别的输入,这意味着  
$y_{i} \cong \widetilde{Y}_ {i}=S\left(F\left(x_{i}^{1}\right), \ldots, F\left(x_{i}^{T}\right) ; \theta\right)$ --(4)  
用Gated Recurrent Neural Network建模S(x)。由于我们将概率分布用于基于帧的分类作为特征表示，我们期望S(x)可以由浅层结构很好地建模。我们的帧到序列模型的架构由具有128个隐藏层的单个门控循环单元（GRU）层和一个softmax层组成。 总体框架如图3所示。
![图3](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Fig3.png)  
##### 3.4. Model Training  
小的面部表情识别数据集通常有几百图像序列，在模型训练时很容易导致过拟合问题。对于模型训练，我们使用水平反转和随机移动(random shifting)的在线数据增强。在文章中，设置最大迭代次数2000epoches并报告用于训练的CNN模型的最佳验证准确性。对于帧到序列模型，使用ADAM优化器作为模型训练并使用48batch大小和固定学习率0.01迭代10000次。  
#### 4. Experiments on Standard Databases  
标准面部表情识别数据库一般包含从中性表情到峰值表情的视频序列。对于基于帧的方法，我们只使用峰值图像作为训练和验证。我们首先在两个知名标准数据库上评估提出的架构：the Extended Cohn-Kanade (CK+) database and the Oulu-CASIA database。CK+数据库由327个标记有7种情感的图像序列组成：anger,contempt, disgust, fear, happiness, sadness, and surprise。Oulu-CASIA数据库包含480个有六种情感标签的图像序列组成：anger,disgust, fear, happiness,sadness, surprise。 CK+ and Oulu-CASIA数据的大小分别是640x490和320x320.这些数据库的详细信息在表4.对于CNN模型的训练，所有加权层通过xavier初始化，学习率固定为0.001，动量设为0.9 。权重衰减方法也用于正则化，系数为0.001 。  
##### 4.1. Frame-based Approach  
为了避免受试者同时出现在训练和测试集中，我们按照ID升序将其分为10个子集，这与10折交叉验证方案相同。该实验方案用于本文实验比较中包含的所有方法。   表1显示了CK +数据库上10折交叉验证的总体准确性。我们基于框架的方法的准确性优于大多数基于序列的方法，仅次于峰引导方法。然而，在[47]中，它们使用CK+ 数据库一千倍大小的500K张图像预训练CNN模型。结果表明提出的CNN模型非常适合在小的数据库上学习面部表情。![表1](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Table1.png)  
在Oulu-CASIA数据库上的表情识别更具挑战性，因为它包含了更多低亮度表情，很难区分低分辨率图像。然而，Oulu-CASIA数据库仍然是好的基准，每个类别有着完整的表情样本。我们的方法的结果优于在Oulu-CASIA数据库上的所有其他方法，如表2所示。![表2](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Table2.png)  
结果表明基于帧的CNN模型相比于最好的方法有4%提升。我们框架的另一个优势是，这两个数据库之间的绝对性能差异只有8.62％，而最先进的方法[47]的性能差异为14.71％。Oulu-CASIA数据库具有很高的识别精度，表明所提出的框架可以在更严格的情况下保持其判别力。 此外，我们的基于帧的方法可以进一步扩展到基于序列的方法，以通过利用时态信息来提高识别准确性。  
##### 4.2. Frame-to-Sequence Approach  
为了进一步利用时间信息并提高识别准确率，我们开发了一种帧到序列方法，该方法使用多个图像帧作为输入，然后根据整个输入序列生成单个预测。  
为了避免由于序列长度偏差引起的问题，我们在每个原始图像序列中执行系统的均匀采样，以将所有训练图像序列归一化为固定长度9，这也是Oulu-CASIA数据库中最短的序列长度。  
对于模型训练，如上一节所述，首先对样本序列进行镜像和随机裁剪增强，使其增加100倍。之后，我们使用训练好的单帧CNN模型来得到每帧的概率分布，因此，对于Oulu-CASIA数据集，每个图像序列都表示为6×9矩阵，对于CK+数据集，则表示为7×9。即，模型每次获取单个帧的特征向量，然后在接收到第9个输入后给出分类决策。 请注意，所有帧到序列模型都使用独立的CNN模型作为特征提取器，这些模型仅使用训练数据进行训练。 也就是说，针对基于序列的方法执行的实验仍遵循标准的10折交叉验证方案。  
如表1和2所示，提出的帧到序列方法实际上提高了CK +和Oulu-CASIA数据库的性能。由于Oulu-CASIA数据库中存在更多的弱表达样本，很难仅从最后一帧区分，因此Oulu-CASIA数据库的改进远远超过CK+数据库。即使我们的方法仍然仅次于CK +数据库上的峰引导方法，但我们的方法在CK +和Oulu-CASIA之间的性能差距已减小到6.8％。 这表明我们的方法是在标准设置中用于面部表情识别的更通用的解决方案。  
##### 4.3. Implementation for Real-World Applications  
在上一节中，我们证明了该方法在准确性和泛化方面均优于以前的方法。 但是，考虑到实际应用，不可避免的问题是硬件存储和计算能力的限制。 由标准数据库开发的系统很难在实践中应用，因为几乎所有面部图像都是这些数据集中的正面面部。  
##### 4.4. Parameter Efficiency  
为了客服硬件存储限制，我们进一步减少我们CNN结构中卷积核的个数。提出的CNN模型的小版本仅使用16个滤波器在前两个卷积层和32个滤波器在后两个卷积层，是原先版本滤波器数量的25%和50%。重复与之前描述的相同的10折验证，结果如表三所示 ![表三](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Table3.png)。即使此修改会导致精度下降，与原版本相比，小模型仅使用50%参数，比最好的CNN模型少80%。少量的参数使该微型模型适用于具有适中存储大小的便携式设备或物联网应用。 微型模型的平均推断时间也从单个NVIDIA GTX 970 GPU的21ms减少到了16ms。  
#### 5. Experiments on Self-Collected Databases
面部表情数据集的收集很昂贵而且消耗时间。研究指出，使用从Internet下载的图像有助于对表情识别问题进行模型训练。为此，我们收集了三个额外的数据集来改善面部表情识别的训练，每个数据集代表特定的数据源。此外，为了防止主观注释，我们收集的每个数据集都用不同的方法标记以确保注释质量。我们收集的所有数据集均由六个常见表情（如Oulu-CASIA数据库[46]）和一个附加的中性表情组成，因为中性面孔最常出现。这些数据集在下文中分别称为集合A，集合B和集合C。  
收集集合A代表笔记本电脑场景中的应用程序。在此数据集中，我们有26个受试者，每个受试者都被要求坐在实验室的椅子上，并观看一系列视频，这些视频导致受试者脸上的表情各异。通过三个网络摄像头同时从不同的近前视角记录了整个观看过程，以丰富头部姿势的变化。 在受试者完成任务后，我们要求他们自己注释时间间隔和相关的表情。然后，我们根据对象的标签清理这些视频剪辑，以仅提取具有峰值表情的帧。 为确保所有表情的数据服从均匀分布，我们为每个类别采样了50张图像。  
集合B是从Google图片搜索引擎收集的，其中包含愤怒的面孔或中性的面孔等关键字，然后使用所用的关键字对搜索结果进行注释。与A组相比，从Internet下载的图像通常具有更多的头部姿势变化，遮挡甚至水印，这可能会使系统从标准角度进行训练数据集很容易失败。  
C组中的图像是从电影，戏剧，新闻或电视节目中收集的，然后根据其故事情节或场景对这些图像进行标记。即使集合C也像集合B一样从Internet收集，它们也有很大的不同。从电影中捕获的人脸图像通常包含强烈的照明对比度和较大的头部姿势变化，这使得该数据集中的样本比集合B更为复杂。我们在图4中描述了附加数据库的一些示例，这些数据库的列表在表4中给出。  
![图4](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Fig4.png)
![表4](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Table4.png)
##### 5.1. Leave-One-Set-Out Experiments  
泛化能力描述了框架在实践中的可行性。 为了研究具有未知数据类型的CNN模型泛化的难度，我们使用具有混合数据类型的数据集训练了我们的小模型，然后在另一种数据类型上进行了泛化评估，这被称为留一法实验。我们在Oulu-CASIA数据库中选择愤怒类别的第一帧作为其中性样本，以便可以在以下实验中使用。此实验中还包括另一个高质量且姿势良好的数据库TFEID。  
在“一站式”实验中，我们使用我们的微型模型进行评估，该模型具有相同的超参数设置，并且具有前面所述的10折验证部分。实验结果显示在表5的第一行中。TFEID数据库的高验证准确性表明，使用混合数据类型进行模型训练仍可以学习一种可以很好地推广到理想情况的表示形式，例如具有强烈表情的正面人脸图像。Oulu-CASIA数据库的性能低下可能是由于训练数据中的偏差，该数据主要由亚洲人的面部样本组成。 然而，在模型训练中我们可以通过适当的增强策略来提高识别的准确性。  
![表5](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Table5.png)
在“一站式”实验中，我们使用我们的微型模型进行评估，该模型具有相同的超参数设置，并且具有前面所述的10折验证部分。实验结果显示在表5的第一行中。TFEID数据库的高验证准确性表明，使用混合数据类型进行模型训练仍可以学习一种可以很好地推广到理想情况的表示形式，例如具有强烈表情的正面人脸图像。Oulu-CASIA数据库的性能低下可能是由于训练数据中的偏差，该数据主要由亚洲人的面部样本组成。 然而，在模型训练中我们可以通过适当的增强策略来提高识别的准确性。  
##### 5.2. Illumination Normalization  
从Internet收集的图像数据通常带有多种照明条件，可能会阻碍模型训练。 亮度标准化被广泛用于各种计算机视觉任务中。 我们采用直方图均衡[31]和线性映射，该映射通过线性变换将最小和最大像素值映射到区间[0，1]，以进行比较。但是，直接应用直方图均衡化可能会过分强调局部对比度，如图5所示，并且当图像已经具有较大的全局对比度时，线性映射不能很好地工作。 因此，我们提出一种加权求和方法来利用两种归一化方法：  
$I_{w s}(x, y)=(1-\lambda) \times I_{h e}(x, y)+\lambda \times I_{l m}(x, y)$ ,$lambda$是权重系数,决定从直方图均衡图像$I_{h e}$和线性映射图像$I_{l m}$占组合图像$I_{w s}的像素比例，在实现中将${lambda}$设置为0.5。一些结果显示在图5的最右列中。尽管线性映射在所有这些方法中带来了最大的性能提升，但使用集合B作为验证时，加权求和方法仍可实现最高的改进，约为6.5％。
![图5](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Fig5.png)  
##### 5.3. Illumination Augmentation
为了利用所有上述归一化方法，一个直观的想法是将它们全部用作训练数据以形成光照增强方法。为了进行验证，我们仍然仅使用上述一种标准化方法来使整个框架在分类过程中保持简洁。 如表5的下面四行所示，验证集无论采用哪种标准化方法，使用提出的光照增强策略实际上都会提高总体准确性。![表5](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Table5.png)  
更有趣的是，如果我们仅应用一种归一化策略，则直方图均衡化方法是唯一会降低平均预测准确性的方法。但是，当我们采用提出的光照增强策略并且仅对验证数据应用直方图均衡时，我们将获得最大的性能提升。对此现象的一种可能解释是，混合光照归一化方法可以促进CNN模型学习到的画像（representation），在训练过程中可以更好地抵抗光照变化。  
#### 6. Experiments on ”In The Wild” Databases
在两个公开wild数据集（MPLab-GENKI数据集和the newest Real-world Affective Faces (RAF)数据集）上测试所提出的框架。如表4所示，由于RAF数据库存在严重的数据失衡问题，因此我们以较少的样本复制了班级中的图像数量，因此模型在训练过程中会更频繁地看到那些稀有样本。  
对于GENKI数据集，我们的模型通过使用先前工作中使用的4折交叉验证方案实现了95.33％的平均准确度和0.34％的标准偏差，如表7所示。即使GENKI数据集只有两类：微笑和非微笑，它仍然是具有挑战性的数据集，其面部部分的分辨率非常低。检测到的最小脸部图像仅具有约20 x 20像素的分辨率。我们发现，提出的光明增强策略无法提高精度，因为GENKI数据集中的大多数图像都是在光照良好的条件下捕获的，并且在该数据集上的精度已经接近饱和。 但是，通过我们的方法获得的结果仍是最好的，无需进一步调整即可获得。  
对于RAF数据库，我们的模型在其评估指标下达到了65.52％的准确度，通过提出的光照增强策略，该性能可以进一步提高到67.55％。我们的结果具有竞争力，因为VGG和AlexNet模型的准确率分别达到58.22％和55.60％。 但是，我们的模型尺寸要小得多。 它仅是前两个模型的5％。 即使我们的结果不如DLP-CNN [32]所报告的最佳准确性74.20％，但是我们的模型使用的参数却少了87.45％。 此外，所有其他模型仅用于特征提取，并且需要额外的多类SVM作为分类器，而我们的CNN模型提供了端到端的表情识别系统。如表6所示，与直接训练相比，使用光照增强的训练在厌恶和恐惧类别上均能达到更高的准确性，它们通常更难以分类。平均精度的提高和不同情感表现下标准偏差的减少也表明，提出的光照增强策略可以帮助CNN模型学习更通用的特征表示，如我们在上一节中提到的。
![]()
![表7](https://github.com/David-on-Code/Facial-expression-recognizition/blob/master/Compact_DL_FER/Table7.png)





[47]X. Zhao, X. Liang, L. Liu, T. Li, Y. Han, N. Vasconcelos,and S. Yan. Peak-piloted deep network for facial expression recognition. In European Conference on Computer Vision,pages 425–442. Springer, 2016.
